{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Denis-R-V/Homeworks/blob/main/Part_5/NLP1_homework.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание NLP1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH7qx_irU4Y8"
      },
      "source": [
        "###ML1_1: \n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_2: \n",
        "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_3: \n",
        "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
        "\n",
        "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 5 семинара: Подсчитайте количество разных слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: bokeh in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (3.0.2)\n",
            "Requirement already satisfied: umap-learn in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (0.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from gensim) (6.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from gensim) (1.8.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from gensim) (1.23.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (6.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (6.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (9.2.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (21.3)\n",
            "Requirement already satisfied: contourpy>=1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (1.0.6)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (2022.9.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (3.1.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from bokeh) (1.4.4)\n",
            "Requirement already satisfied: numba>=0.49 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from umap-learn) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from umap-learn) (0.5.8)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from umap-learn) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh) (2.1.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from numba>=0.49->umap-learn) (65.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from packaging>=16.8->bokeh) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ds_env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.2->bokeh) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#Установка нужных пакетов\n",
        "!pip install --upgrade nltk gensim bokeh umap-learn\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import umap\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-04 19:42:27--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
            "Распознаётся www.dropbox.com (www.dropbox.com)… 162.125.70.18\n",
            "Подключение к www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
            "Адрес: /s/dl/obaitrix9jyu84r/quora.txt [переход]\n",
            "--2022-12-04 19:42:27--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n",
            "Повторное использование соединения с www.dropbox.com:443.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
            "Адрес: https://uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com/cd/0/get/Bx_rMZVMQOdSe_YXK_JNkSm0apoHJqyIcqJgy-ql0-2pW4dF0BRgBEU4ZQP2923hykXxQUYOuYhMOW2DuquA_IztKnQM_b_kR8_AOGOS3-mnHqlrMMYvOaXYTqk0M7d51B1K6FRxIhTpL6WAaR_tK_2fOaMj7M-gm9ISWO9PzuAbRA/file?dl=1# [переход]\n",
            "--2022-12-04 19:42:28--  https://uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com/cd/0/get/Bx_rMZVMQOdSe_YXK_JNkSm0apoHJqyIcqJgy-ql0-2pW4dF0BRgBEU4ZQP2923hykXxQUYOuYhMOW2DuquA_IztKnQM_b_kR8_AOGOS3-mnHqlrMMYvOaXYTqk0M7d51B1K6FRxIhTpL6WAaR_tK_2fOaMj7M-gm9ISWO9PzuAbRA/file?dl=1\n",
            "Распознаётся uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com (uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com)… 162.125.70.15\n",
            "Подключение к uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com (uc9e4747322b09d15bcc3d210d20.dl.dropboxusercontent.com)|162.125.70.15|:443... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
            "Длина: 33813903 (32M) [application/binary]\n",
            "Сохранение в: «./quora.txt»\n",
            "\n",
            "./quora.txt         100%[===================>]  32,25M  6,36MB/s    за 5,0s    \n",
            "\n",
            "2022-12-04 19:42:34 (6,49 MB/s) - «./quora.txt» сохранён [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# выгружаем датасет:\n",
        "!wget \"https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\" -O ./quora.txt -nc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"What TV shows or books help you read people's body language?\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = list(open(\"quora.txt\", encoding=\"utf-8\"))\n",
        "data[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xJfkstKpqsXp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_tok = list()\n",
        "for a in data:\n",
        "    current_token = tokenizer.tokenize(a)\n",
        "    row = list()\n",
        "    for b in current_token:\n",
        "        row.append(b.lower())\n",
        "    data_tok.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/denis/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/denis/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество унакальных слов после лемматизации и стемминга: 66835\n",
            "Количество унакальных слов после стемминга и лемматизации: 66818\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for i in data_tok:\n",
        "  for j in i:\n",
        "    all_words.append(j)\n",
        "\n",
        "# Лемматизация - стемминг\n",
        "lem_stem = []\n",
        "for i in all_words:\n",
        "    lem_stem.append(ps.stem(lemmatizer.lemmatize(i)))\n",
        "\n",
        "# Стемминг лмматизация\n",
        "stem_lem = []\n",
        "for i in all_words:\n",
        "    stem_lem.append(lemmatizer.lemmatize(ps.stem(i)))\n",
        "\n",
        "print(f\"Количество унакальных слов после лемматизации и стемминга: {len(set(lem_stem))}\")\n",
        "print(f\"Количество унакальных слов после стемминга и лемматизации: {len(set(stem_lem))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML1_1: \n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true\n"
          ]
        }
      ],
      "source": [
        "Regex_Pattern = r'(ok){3}'\t# Do not delete 'r'.\n",
        "\n",
        "import re\n",
        "test_string = 'okokok! cya'\n",
        "print(str(bool(re.search(Regex_Pattern, test_string))).lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML1_2: \n",
        "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Питон не дружит с Branch reset group\n",
        "\n",
        "# $Regex_Pattern = '^\\d{2}(?|(---)|(-)|(.)|(:))\\d{2}\\1\\d{2}\\1\\d{2}$';\n",
        "\n",
        "# $Test_String = <STDIN> ;\n",
        "# if($Test_String =~ /$Regex_Pattern/){\n",
        "#     print \"true\";\n",
        "# } else {\n",
        "#     print \"false\";\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML1_3: \n",
        "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "input\n",
        "2\n",
        "<p><a href=\"http://www.quackit.com/html/tutorial/html_links.cfm\">Example Link</a></p>\n",
        "<div class=\"more-info\"><a href=\"http://www.quackit.com/html/examples/html_links_examples.cfm\">More Link Examples...</a></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://www.quackit.com/html/tutorial/html_links.cfm,Example Link\n",
            "http://www.quackit.com/html/examples/html_links_examples.cfm,More Link Examples...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "Regex_Pattern = r'(?<=<a href=\")([^\"]+)\".*?>([^<]*)(?=</)'\n",
        "\n",
        "for i in range(int(input())):\n",
        "    matches = re.findall(Regex_Pattern, str(input()))\n",
        "    if matches:\n",
        "        for match in matches:\n",
        "            print(match[0],match[1].strip(),sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP1_homework",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('ds_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "412736b7ad9a6211ebe07fddd452488530ba48e4d95dcf46d993658ed665f40c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
